{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔑 Part 4 연습문제 정답"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfvJjIp17Pvv"
      },
      "source": [
        "##### ✏️ 연습문제 1: 최적의 `n_components` 찾기\n",
        "\n",
        "PCA를 사용할 때, 정보 손실을 최소화하면서 차원을 얼마나 줄일지 결정하는 것이 중요합니다. \n",
        "\n",
        "`PCA`의 `explained_variance_ratio_` 속성을 이용하여 **분산의 95%를 설명하기 위해 몇 개의 주성분이 필요한지** 계산하고, 결과를 시각화해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUG0kzZM7Pvv"
      },
      "outputs": [],
      "source": [
        "# 연습문제 1 코드\n",
        "# StandardScaler로 전체 샘플 데이터(X_sample) 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_sample_scaled = scaler.fit_transform(X_sample)\n",
        "\n",
        "# PCA 객체 생성 (모든 주성분을 확인하기 위해 n_components를 설정하지 않음)\n",
        "pca = PCA()\n",
        "pca.fit(X_sample_scaled)\n",
        "\n",
        "# 누적 설명 분산 계산\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# 95% 분산을 설명하는 지점 찾기\n",
        "# 여기에 코드를 작성하여 n_components_95를 구하세요.\n",
        "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "print(f\"95%의 분산을 설명하는 주성분 개수: {n_components_95}\")\n",
        "\n",
        "\n",
        "# 누적 설명 분산 시각화\n",
        "fig = px.area(\n",
        "    x=range(1, cumulative_variance.shape[0] + 1),\n",
        "    y=cumulative_variance,\n",
        "    labels={\"x\": \"주성분 개수\", \"y\": \"누적 설명 분산 비율\"},\n",
        "    title=\"PCA 누적 설명 분산\"\n",
        ")\n",
        "fig.add_shape(type=\"line\", x0=0, y0=0.95, x1=n_components_95, y1=0.95, line=dict(color=\"Red\", dash=\"dash\"))\n",
        "fig.add_shape(type=\"line\", x0=n_components_95, y0=0, x1=n_components_95, y1=0.95, line=dict(color=\"Red\", dash=\"dash\"))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AHT20Oc7Pvv"
      },
      "source": [
        "##### ✏️ 연습문제 2: `GridSearchCV`로 최적의 SVM 파라미터 찾기\n",
        "\n",
        "`Pipeline`과 `GridSearchCV`를 함께 사용하면 전처리 파라미터와 모델 하이퍼파라미터를 동시에 최적화할 수 있습니다.\n",
        "\n",
        " 위에서 구한 `n_components_95`를 PCA에 적용하고, SVM의 `C`와 `gamma`에 대한 최적값을 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5TUP5Io7Pvv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 연습문제 2 코드\n",
        "# 1. 파이프라인 정의\n",
        "# n_components는 연습문제 1에서 구한 값으로 설정\n",
        "pipe_for_grid = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=n_components_95)), # n_components_95 값 사용\n",
        "    ('svm', SVC(kernel='rbf', random_state=42))\n",
        "])\n",
        "\n",
        "# 2. 탐색할 하이퍼파라미터 그리드 정의\n",
        "# 파이프라인의 각 단계 이름과 파라미터 이름을 '__'로 연결\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10],\n",
        "    'svm__gamma': ['scale', 'auto', 0.1, 1]\n",
        "}\n",
        "\n",
        "# 3. GridSearchCV 객체 생성 및 학습\n",
        "# cv=3 (3-fold cross-validation)\n",
        "# n_jobs=-1 (사용 가능한 모든 CPU 코어 사용)\n",
        "grid_search = GridSearchCV(pipe_for_grid, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 4. 최적 파라미터 및 성능 출력\n",
        "print(\"최적 하이퍼파라미터:\", grid_search.best_params_)\n",
        "print(\"최고 교차 검증 점수:\", grid_search.best_score_)\n",
        "print(\"테스트 세트 점수:\", grid_search.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0kSFOb-7Pvv"
      },
      "source": [
        "##### ✏️ 연습문제 3: 사전 확률(Prior) 확인하기\n",
        "\n",
        "학습된 `GaussianNB` 모델이 데이터로부터 학습한 각 클래스의 \\*\\*사전 확률(Prior Probability)\\*\\*을 확인해보세요. `gnb_pipeline`의 `classifier` 단계에 접근하여 `class_prior_` 속성을 출력하면 됩니다. 이 값은 전체 훈련 데이터의 클래스 비율과 유사할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT6vwMzE7Pvw"
      },
      "outputs": [],
      "source": [
        "# 연습문제 3 코드\n",
        "# 파이프라인에서 학습된 모델 접근\n",
        "trained_gnb = gnb_pipeline.named_steps['classifier']\n",
        "\n",
        "# 사전 확률 출력\n",
        "# 여기에 코드를 작성하세요.\n",
        "prior_probabilities = trained_gnb.class_prior_\n",
        "print(f\"클래스 0 (연체X)의 사전 확률: {prior_probabilities[0]:.4f}\")\n",
        "print(f\"클래스 1 (연체O)의 사전 확률: {prior_probabilities[1]:.4f}\")\n",
        "\n",
        "# 훈련 데이터의 실제 클래스 비율과 비교\n",
        "print(\"\\n훈련 데이터의 실제 클래스 비율:\")\n",
        "print(y_train.value_counts(normalize=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
