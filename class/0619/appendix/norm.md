# Ridge(릿지) 회귀와 Lasso(라쏘) 회귀

**Ridge(릿지) 회귀**와 **Lasso(라쏘) 회귀**는 선형 회귀(Linear Regression)의 한계를 보완하기 위해 고안된 정규화(regularization) 기반 회귀 기법입니다. 두 방법 모두 과적합(overfitting)을 방지하고, 모델의 일반화 성능을 높이기 위해 회귀계수에 패널티(penalty)를 부여합니다.

---

## Ridge(릿지) 회귀 (L2 Regularization)

- 릿지 회귀는 손실 함수에 회귀계수의 제곱합을 패널티로 추가합니다.
- 수식: 
$$
\text{minimize} \quad \text{SSE} + \lambda \sum_{j=1}^p \beta_j^2
$$
- 여기서 $\lambda$는 패널티의 강도를 조절하는 하이퍼파라미터입니다.
- 계수 전체를 0에 가깝게 축소하지만, 완전히 0으로 만들지는 않습니다.
- 모든 변수를 남기면서 계수 크기만 줄이므로, 변수 선택(feature selection) 효과는 없습니다.
- 다중공선성 문제(독립변수 간 상관관계가 높을 때) 완화에 효과적입니다.

---

## Lasso(라쏘) 회귀 (L1 Regularization)

- 라쏘 회귀는 손실 함수에 회귀계수의 절댓값 합을 패널티로 추가합니다.
- 수식: 
$$
\text{minimize} \quad \text{SSE} + \lambda \sum_{j=1}^p |\beta_j|
$$
- $\lambda$ 값이 커질수록 일부 계수가 정확히 0이 되면서, 중요하지 않은 변수는 모델에서 자동으로 제외됩니다.
- 즉, 변수 선택(feature selection) 효과가 있습니다.
- 입력 변수 중 영향력이 큰 소수만 남기고, 나머지는 제거하고 싶을 때 효과적입니다.

---

## 주요 차이점 비교

| 구분         | Ridge(릿지) 회귀         | Lasso(라쏘) 회귀         |
|--------------|--------------------------|--------------------------|
| 패널티 종류  | 계수의 제곱합(L2 norm)   | 계수의 절댓값 합(L1 norm)|
| 계수 축소    | 0에 근접, 완전 0은 아님  | 일부 계수 완전 0 가능    |
| 변수 선택    | 불가능                   | 가능 (feature selection) |
| 사용 상황    | 모든 변수 활용, 다중공선성 완화 | 변수 중 일부만 중요할 때 |
