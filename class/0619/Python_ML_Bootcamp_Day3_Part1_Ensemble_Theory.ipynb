{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZzJTfAqTmbo"
      },
      "source": [
        "# 🚀 Day 3-1: 앙상블 기법의 정석 (Ensemble Deep-Dive)\n",
        "\n",
        "\"하나의 머리보다는 여러 개의 머리가 낫다.\" (Many heads are better than one.)\n",
        "\n",
        "이 오랜 격언은 머신러닝 세계에서도 황금률과 같습니다. \n",
        "\n",
        "아무리 뛰어난 단일 모델이라도 예측할 수 없는 맹점(blind spot)이 존재하기 마련입니다. \n",
        "\n",
        "하지만 여러 모델의 '의견'을 종합하면, 개별 모델의 약점은 상쇄되고 강점은 극대화되어 훨씬 더 안정적이고 강력한 예측 성능을 이끌어낼 수 있습니다. \n",
        "\n",
        "이처럼 <u>여러 개의 모델(Classifier 또는 Regressor)을 결합하여 단일 모델보다 뛰어난 성능을 내는 기법</u>을 <u>앙상블(Ensemble)</u> 학습이라고 합니다.\n",
        "\n",
        "이번 시간에는 앙상블의 가장 기본적이면서도 핵심적인 네 가지 아이디어, <u>Voting, Bagging, Boosting, Stacking</u>에 대해 깊이 있게 알아봅니다.\n",
        "\n",
        " 각 기법이 어떤 원리로 동작하며, `scikit-learn`을 통해 어떻게 구현하는지, 그리고 이들을 통해 어떻게 모델의 <u>편향(Bias)과 분산(Variance)</u> 을 효과적으로 제어할 수 있는지 배우게 될 것입니다.\n",
        "\n",
        "이번 실습에서는 Kaggle의 <u>심부전 예측(Heart Failure Prediction) 데이터셋</u>을 사용하여 환자의 생존 여부를 예측하는 분류 문제를 해결하며 앙상블 기법의 위력을 직접 체험해 보겠습니다.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 데이터 및 기본 모델 준비\n",
        "\n",
        "앙상블 기법을 실습하기 위해, 먼저 데이터를 불러오고 앙상블에 사용할 여러 기본 모델(Base Models)을 준비하겠습니다.\n",
        "\n",
        "#### 💻 코드로 알아보기\n",
        "\n",
        "[Kaggle: Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)에서 `heart.csv` 파일을 다운로드하여 실습에 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_mCLOG8GTmbr"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>anaemia</th>\n",
              "      <th>creatinine_phosphokinase</th>\n",
              "      <th>diabetes</th>\n",
              "      <th>ejection_fraction</th>\n",
              "      <th>high_blood_pressure</th>\n",
              "      <th>platelets</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>serum_sodium</th>\n",
              "      <th>sex</th>\n",
              "      <th>smoking</th>\n",
              "      <th>time</th>\n",
              "      <th>DEATH_EVENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75.0</td>\n",
              "      <td>0</td>\n",
              "      <td>582</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>265000.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7861</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>263358.03</td>\n",
              "      <td>1.1</td>\n",
              "      <td>136</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65.0</td>\n",
              "      <td>0</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>162000.00</td>\n",
              "      <td>1.3</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.0</td>\n",
              "      <td>1</td>\n",
              "      <td>111</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>210000.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>137</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65.0</td>\n",
              "      <td>1</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>327000.00</td>\n",
              "      <td>2.7</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
              "0  75.0        0                       582         0                 20   \n",
              "1  55.0        0                      7861         0                 38   \n",
              "2  65.0        0                       146         0                 20   \n",
              "3  50.0        1                       111         0                 20   \n",
              "4  65.0        1                       160         1                 20   \n",
              "\n",
              "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
              "0                    1  265000.00               1.9           130    1   \n",
              "1                    0  263358.03               1.1           136    1   \n",
              "2                    0  162000.00               1.3           129    1   \n",
              "3                    0  210000.00               1.9           137    1   \n",
              "4                    0  327000.00               2.7           116    0   \n",
              "\n",
              "   smoking  time  DEATH_EVENT  \n",
              "0        0     4            1  \n",
              "1        0     6            1  \n",
              "2        1     7            1  \n",
              "3        0     7            1  \n",
              "4        0     8            1  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 기본 분류 모델\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 앙상블 모델\n",
        "from sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier, AdaBoostClassifier\n",
        "\n",
        "# 평가 지표\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# 데이터셋 로드 (자신의 파일 경로에 맞게 수정해주세요)\n",
        "path = '../datasets/ml/heart-failure/heart_failure_clinical_records_dataset.csv'\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 준비 완료!\n",
            "훈련 데이터 크기: (239, 12)\n",
            "테스트 데이터 크기: (60, 12)\n"
          ]
        }
      ],
      "source": [
        "# 특성과 타겟 분리\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# 전처리 파이프라인 설정\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "categorical_features = X.select_dtypes(include='object').columns\n",
        "\n",
        "numeric_feature_indices = [X.columns.get_loc(col) for col in numeric_features]\n",
        "categorical_feature_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_feature_indices), # 인덱스 값으로 지정\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_feature_indices)\n",
        "    ])\n",
        "\n",
        "print(\"데이터 준비 완료!\")\n",
        "print(f\"훈련 데이터 크기: {X_train.shape}\")\n",
        "print(f\"테스트 데이터 크기: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX4sCuPNTmbs"
      },
      "source": [
        "---\n",
        "\n",
        "### 2. 투표 (Voting)\n",
        "\n",
        "가장 직관적인 앙상블 방법입니다. 여러 명의 전문가가 투표를 통해 최종 결정을 내리는 것과 같습니다. 각 모델이 독립적으로 예측한 결과를 모아, 다수결 또는 평균으로 최종 결론을 내립니다.\n",
        "\n",
        "#### 🧠 개념 이해하기\n",
        "\n",
        "* <u>하드 보팅 (Hard Voting)</u>: 다수결 원칙입니다. 여러 모델 중 가장 많은 표를 받은 클래스를 최종 예측값으로 선택합니다. '분류(Classification)' 문제에만 사용할 수 있습니다.\n",
        "    * 예: 모델 A가 '생존', 모델 B가 '사망', 모델 C가 '생존'으로 예측했다면, 최종 예측은 '생존'이 됩니다.\n",
        "\n",
        "* <u>소프트 보팅 (Soft Voting)</u>: 각 모델이 예측한 '확률'을 평균 내어, 가장 높은 확률을 가진 클래스를 최종 예측값으로 선택합니다. 일반적으로 하드 보팅보다 성능이 더 좋다고 알려져 있습니다. 각 모델의 예측 신뢰도를 반영할 수 있기 때문입니다.\n",
        "    * 예: 모델 A가 '생존' 확률 80%, 모델 B가 40%, 모델 C가 60%로 예측했다면, 평균 '생존' 확률은 $(0.8+0.4+0.6)/3 = 0.6$ 즉 60%가 되어 최종 예측은 '생존'이 됩니다.\n",
        "    * 모델에 <u>가중치(weights)</u>를 부여하여 특정 모델의 의견에 더 힘을 실어줄 수도 있습니다.\n",
        "\n",
        "#### 💻 코드로 알아보기\n",
        "\n",
        "로지스틱 회귀, K-최근접 이웃, 결정 트리 세 가지 모델을 사용하여 `VotingClassifier`를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vAXFPDnyTmbt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.8167\n",
            "Logistic Regression f1_score: 0.6667\n",
            "KNN Accuracy: 0.7000\n",
            "KNN f1_score: 0.3077\n",
            "Decision Tree Accuracy: 0.7333\n",
            "Decision Tree f1_score: 0.5294\n",
            "Hard Voting Accuracy: 0.7833\n",
            "Hard Voting f1_score: 0.5806\n",
            "Soft Voting Accuracy: 0.7500\n",
            "Soft Voting f1_score: 0.5455\n"
          ]
        }
      ],
      "source": [
        "# 1. 앙상블에 사용할 기본 모델 정의\n",
        "# 각 모델을 파이프라인으로 묶어 전처리가 자동으로 적용되도록 합니다.\n",
        "lr_clf = Pipeline([('preprocessor', preprocessor), ('classifier', LogisticRegression(random_state=42))])\n",
        "knn_clf = Pipeline([('preprocessor', preprocessor), ('classifier', KNeighborsClassifier(n_neighbors=5))])\n",
        "dt_clf = Pipeline([('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42))])\n",
        "\n",
        "# 2. 하드 보팅 (Hard Voting)\n",
        "# 'estimators'에는 (모델명, 모델객체) 튜플의 리스트를 전달\n",
        "voting_clf_hard = VotingClassifier(\n",
        "    estimators=[('lr', lr_clf), ('knn', knn_clf), ('dt', dt_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# 3. 소프트 보팅 (Soft Voting)\n",
        "# voting='soft'로 변경\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', lr_clf), ('knn', knn_clf), ('dt', dt_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "lr_clf.fit(X_train, y_train)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "voting_clf_hard.fit(X_train, y_train)\n",
        "voting_clf_soft.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 각 모델의 정확도 평가\n",
        "models = [('Logistic Regression', lr_clf), ('KNN', knn_clf), ('Decision Tree', dt_clf),\n",
        "          ('Hard Voting', voting_clf_hard), ('Soft Voting', voting_clf_soft)]\n",
        "\n",
        "for name, model in models:\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1_score_ = f1_score(y_test, y_pred)\n",
        "    print(f'{name} Accuracy: {accuracy:.4f}')\n",
        "    print(f'{name} f1_score: {f1_score_:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrDClEtrTmbt"
      },
      "source": [
        "#### ✏️ 연습문제 1\n",
        "\n",
        "소프트 보팅 시, `DecisionTreeClassifier` 모델의 성능이 다른 모델보다 상대적으로 낮다고 가정해봅시다. `VotingClassifier`의 `weights` 파라미터를 사용하여 **로지스틱 회귀에 2, KNN에 2, 결정 트리에 1의 가중치**를 부여한 새로운 소프트 보팅 모델(`voting_clf_weighted`)을 만들고, 정확도를 평가해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cexC3iCGTmbt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weighted Soft Voting Accuracy: 0.7667\n"
          ]
        }
      ],
      "source": [
        "# 연습문제 1 코드\n",
        "\n",
        "# 가중치를 적용한 소프트 보팅 모델을 정의하세요.\n",
        "# 연습문제 1: 소프트 보팅에 가중치 적용\n",
        "voting_clf_weighted = VotingClassifier(\n",
        "    estimators=[('lr', lr_clf), ('knn', knn_clf), ('dt', dt_clf)],\n",
        "    voting='soft',\n",
        "    weights=[2, 2, 1]\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "voting_clf_weighted.fit(X_train, y_train)\n",
        "weighted_accuracy = voting_clf_weighted.score(X_test, y_test)\n",
        "\n",
        "# 정확도 평가\n",
        "print(f'Weighted Soft Voting Accuracy: {weighted_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rzy70zGTmbu"
      },
      "source": [
        "---\n",
        "\n",
        "### 3. 배깅 (Bagging)\n",
        "\n",
        "<u>배깅(Bagging)</u>은 <u>B</u>ootstrap <u>Agg</u>regat<u>ing</u>의 약자로, 같은 알고리즘을 사용하지만 <u>서로 다른 훈련 데이터 샘플</u>로 여러 모델을 학습시켜 그 결과를 종합하는 기법입니다.\n",
        "\n",
        "#### 🧠 개념 이해하기\n",
        "\n",
        "1.  <u>부트스트랩 (Bootstrap)</u>: 원본 훈련 데이터에서 <u>중복을 허용하여</u> 원본 데이터와 같은 크기의 샘플을 여러 개 만드는 과정입니다. (예: [A, B, C, D]에서 샘플링하면 [A, B, B, D]가 나올 수 있습니다.)\n",
        "2.  <u>병렬 학습 (Parallel Training)</u>: 각 부트스트랩 샘플을 사용하여 독립적으로 여러 개의 모델을 <u>동시에(병렬로)</u> 학습시킵니다.\n",
        "3.  <u>결과 종합 (Aggregating)</u>: 각 모델의 예측 결과를 투표(Voting)를 통해 합칩니다.\n",
        "\n",
        "배깅은 각 모델이 약간씩 다른 데이터를 학습하기 때문에, 개별 모델의 예측이 조금씩 달라집니다. 이들의 예측을 평균내면 한쪽으로 치우친 예측(과대적합)을 줄일 수 있어 <u>모델의 분산(Variance)을 낮추는 효과</u>가 매우 큽니다.\n",
        "\n",
        "가장 대표적인 배깅 기반 알고리즘이 바로 <u>랜덤 포레스트(Random Forest)</u>입니다. 랜덤 포레스트는 결정 트리(Decision Tree)를 기본 모델로 사용하며, 부트스트랩뿐만 아니라 <u>각 노드에서 특성을 무작위로 선택</u>하는 과정을 추가하여 모델 간의 상관관계를 더욱 낮춘, 배깅의 확장판입니다.\n",
        "\n",
        "#### 💻 코드로 알아보기\n",
        "\n",
        "`scikit-learn`의 `BaggingClassifier`를 사용해 결정 트리 모델을 배깅 방식으로 앙상블 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBYd0V7STmbu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging (Decision Tree) Accuracy: 0.8167\n",
            "Single Decision Tree Accuracy: 0.7333\n"
          ]
        }
      ],
      "source": [
        "# BaggingClassifier 사용\n",
        "# estimator: 기본 모델\n",
        "# n_estimators: 앙상블할 모델의 개수\n",
        "# max_samples: 각 모델이 학습할 샘플의 비율\n",
        "# n_jobs=-1: 모든 CPU 코어를 사용하여 병렬 학습\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=Pipeline([('preprocessor', preprocessor),\n",
        "                       ('classifier', DecisionTreeClassifier(random_state=42))]),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8, # 훈련 데이터의 80%를 샘플링\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Bagging 모델 학습\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# 정확도 평가\n",
        "bagging_accuracy = bagging_clf.score(X_test, y_test)\n",
        "print(f'Bagging (Decision Tree) Accuracy: {bagging_accuracy:.4f}')\n",
        "\n",
        "# 참고: 단일 결정 트리 모델의 정확도\n",
        "dt_accuracy = dt_clf.score(X_test, y_test)\n",
        "print(f'Single Decision Tree Accuracy: {dt_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBga3hG-Tmbu"
      },
      "source": [
        "#### ✏️ 연습문제 2\n",
        "\n",
        "`BaggingClassifier`의 기본 모델(`base_estimator`)을 `LogisticRegression`으로 변경하여 새로운 배깅 모델(`bagging_lr_clf`)을 만들어 보세요. `n_estimators`는 30개로 설정하고, 학습 후 정확도를 평가하여 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjXvVXDHTmbu"
      },
      "outputs": [],
      "source": [
        "# 연습문제 2 코드\n",
        "\n",
        "# 로지스틱 회귀를 기본 모델로 사용하는 BaggingClassifier를 정의하세요.\n",
        "# 연습문제 2: BaggingClassifier의 estimator를 LogisticRegression으로 변경\n",
        "bagging_lr_clf = BaggingClassifier(\n",
        "    estimator=Pipeline([('preprocessor', preprocessor),\n",
        "                             ('classifier', LogisticRegression(random_state=42))]),\n",
        "    n_estimators=30,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "bagging_lr_clf.fit(X_train, y_train)\n",
        "lr_bagging_accuracy = bagging_lr_clf.score(X_test, y_test)\n",
        "\n",
        "# 정확도 평가\n",
        "print(f'Bagging (Logistic Regression) Accuracy: {lr_bagging_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxIMkYwGTmbv"
      },
      "source": [
        "---\n",
        "\n",
        "### 4. 부스팅 (Boosting)\n",
        "\n",
        "<u>부스팅(Boosting)</u>은 여러 개의 약한 학습기(Weak Learner)를 <u>순차적으로</u> 학습시켜, 이전 모델이 잘못 예측한 데이터에 가중치를 부여하며 점점 더 강력한 모델을 만들어가는 기법입니다.\n",
        "\n",
        "#### 🧠 개념 이해하기\n",
        "\n",
        "1.  첫 번째 모델이 전체 데이터를 학습하고 예측합니다.\n",
        "2.  <u>예측이 틀린 데이터에 가중치</u>를 높입니다. (더 중요하게 만듭니다.)\n",
        "3.  두 번째 모델은 가중치가 적용된 데이터를 학습합니다. 즉, 이전 모델이 어려워했던 문제에 더 집중하게 됩니다.\n",
        "4.  이 과정을 반복하며, 각 모델은 이전 모델의 실수를 보완하는 방향으로 학습됩니다.\n",
        "5.  최종 예측은 모든 모델의 예측을 가중합하여 결정합니다.\n",
        "\n",
        "부스팅은 잘못된 부분을 집중적으로 개선해 나가기 때문에 <u>모델의 편향(Bias)을 낮추는 데</u> 매우 효과적입니다. 대표적인 부스팅 알고리즘으로는 <u>AdaBoost, Gradient Boosting, XGBoost, LightGBM</u> 등이 있습니다.\n",
        "\n",
        "#### 💻 코드로 알아보기\n",
        "\n",
        "가장 기본적인 부스팅 알고리즘 중 하나인 `AdaBoostClassifier`를 사용해 보겠습니다. AdaBoost는 결정 트리를 기본 모델로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pOziWBJ-Tmbv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost Accuracy: 0.7667\n"
          ]
        }
      ],
      "source": [
        "# AdaBoostClassifier 사용\n",
        "# AdaBoost는 내부적으로 전처리를 직접 처리하기 어려우므로,\n",
        "# 전처리된 데이터를 직접 입력해야 합니다.\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# AdaBoost 모델 정의\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# AdaBoost 모델 학습\n",
        "adaboost_clf.fit(X_train_processed, y_train)\n",
        "\n",
        "# 정확도 평가\n",
        "adaboost_accuracy = adaboost_clf.score(X_test_processed, y_test)\n",
        "print(f'AdaBoost Accuracy: {adaboost_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwAzQ05Tmbv"
      },
      "source": [
        "#### ✏️ 연습문제 3\n",
        "\n",
        "`AdaBoostClassifier`에서 학습률(`learning_rate`)은 각 모델이 이전 모델의 오류를 얼마나 강하게 보정할지를 결정하는 중요한 파라미터입니다. `learning_rate`를 `1.0`으로 변경하여 새로운 AdaBoost 모델(`adaboost_clf_lr1`)을 만들고, 학습 후 정확도를 평가해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqXB_YTNTmbv"
      },
      "outputs": [],
      "source": [
        "# 연습문제 3 코드\n",
        "\n",
        "# learning_rate가 1.0인 AdaBoost 모델을 정의하세요.\n",
        "# 연습문제 3: AdaBoostClassifier의 learning_rate를 1.0으로 변경\n",
        "adaboost_clf_lr1 = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "adaboost_clf_lr1.fit(X_train_processed, y_train)\n",
        "ada_lr1_accuracy = adaboost_clf_lr1.score(X_test_processed, y_test)\n",
        "\n",
        "# 정확도 평가\n",
        "print(f'AdaBoost (learning_rate=1.0) Accuracy: {ada_lr1_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2rLOPfITmbv"
      },
      "source": [
        "---\n",
        "\n",
        "### 5. 스태킹 (Stacking)\n",
        "\n",
        "<u>스태킹(Stacking)</u>은 여러 다른 종류의 모델(Base Models)의 예측값을 <u>특성(feature)처럼</u> 사용하여, 이 예측값들을 학습하는 새로운 모델(Meta Model)을 통해 최종 예측을 수행하는 복잡하고 강력한 앙상블 기법입니다.\n",
        "\n",
        "#### 🧠 개념 이해하기\n",
        "\n",
        "1.  <u>Level 0 (Base Models)</u>: 여러 개의 기본 모델(e.g., 로지스틱 회귀, KNN, 결정 트리)이 훈련 데이터를 학습하고 예측값을 만듭니다.\n",
        "2.  <u>Level 1 (Meta Model)</u>: Level 0에서 생성된 예측값들을 <u>입력 특성</u>으로 받아서 학습하는 최종 모델입니다. 이 메타 모델은 각 기본 모델의 예측 결과를 어떻게 조합해야 최적의 결과를 낼 수 있는지를 학습합니다.\n",
        "\n",
        "스태킹은 마치 \"각 분야 전문가(Base Models)들의 소견을 듣고 최종 결정을 내리는 관리자(Meta Model)\"와 같습니다. 서로 다른 방식의 모델들을 조합하여 각 모델의 장점만을 취하려는 시도이기 때문에, 종종 캐글(Kaggle)과 같은 데이터 경진대회에서 높은 성능을 달성하기 위해 사용됩니다.\n",
        "\n",
        "<u>중요</u>: 데이터 유출(Data Leakage)을 방지하기 위해, 스태킹에서는 <u>교차 검증(Cross-Validation)</u> 방식을 사용하여 기본 모델들이 예측값을 생성하도록 하는 것이 일반적입니다.\n",
        "\n",
        "#### 💻 코드로 알아보기\n",
        "\n",
        "`scikit-learn`의 `StackingClassifier`를 사용하여 로지스틱 회귀, KNN, 결정 트리를 기본 모델로, 최종 메타 모델은 로지스틱 회귀를 사용하는 스태킹 앙상블을 구축해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tGGT08VrTmbv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stacking Accuracy: 0.8000\n"
          ]
        }
      ],
      "source": [
        "# StackingClassifier 사용\n",
        "# estimators: 기본 모델들 (Voting과 동일한 형식)\n",
        "# final_estimator: 메타 모델\n",
        "# cv: 데이터 유출 방지를 위한 교차 검증 폴드 수\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('lr', lr_clf), ('knn', knn_clf), ('dt', dt_clf)],\n",
        "    final_estimator=LogisticRegression(random_state=42),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Stacking 모델 학습\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# 정확도 평가\n",
        "stacking_accuracy = stacking_clf.score(X_test, y_test)\n",
        "print(f'Stacking Accuracy: {stacking_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akq70b0nTmbw"
      },
      "source": [
        "#### ✏️ 연습문제 4\n",
        "\n",
        "`StackingClassifier`의 메타 모델(`final_estimator`)을 `DecisionTreeClassifier(max_depth=2, random_state=42)`로 변경하여 새로운 스태킹 모델(`stacking_dt_clf`)을 만들고, 학습 후 정확도를 평가해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bzQgiRITmbw"
      },
      "outputs": [],
      "source": [
        "# 연습문제 4 코드\n",
        "\n",
        "# 메타 모델이 DecisionTree인 StackingClassifier를 정의하세요.\n",
        "stacking_dt_clf = StackingClassifier(\n",
        "    estimators=[('lr', lr_clf), ('knn', knn_clf), ('dt', dt_clf)],\n",
        "    final_estimator=?, # 여기에 DecisionTreeClassifier를 넣으세요.\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "# ?\n",
        "\n",
        "# 정확도 평가\n",
        "# ?\n",
        "# print(f'Stacking (Meta=DT) Accuracy: {stacking_dt_accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
